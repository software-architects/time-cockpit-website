---
layout: blog
title: Faster Synchronization in Time Cockpit July 2013
excerpt: Time cockpit can work online or offline. All changes are synchronized to a local database for offline use. For the next version (July 2013, 1.14) we have dramatically improved the performance when syncing large amounts of signal data, greatly reducing initial synchronization times for new users or devices.
author: Philipp Aumayr
date: 2013-06-25
bannerimage: 
lang: en
tags: [.NET,Azure,C#,time cockpit]
permalink: /blog/2013/06/25/Faster-Synchronization-in-Time-Cockpit-July-2013
---

<h2>Synchronization Performance</h2><p>One of the main pain points our customers faced when using time cockpit was the initial time it takes time cockpit to setup. The main reason this took so long was a process called “blob syncing”. The blobs are the containers for the signal data collected by the signal tracker. Each blob is a single small container, usually only a few kilobytes in size and is stored in the blob storage mechanism of the data layer. The advantage of using blobs is that they are encrypted by a user specific password, which we refer to as signal encryption password.</p><p>When we initially designed time cockpit, we did not consider the possibility that users would keep their signal data forever. Rather we expected signals to be saved until the timesheets for the period of time (day, week or possibly a month) are created. We have learned over the past few years that for our customers the default is to keep all signal data rather than to remove it once it is not needed anymore. In case you did not know, you can delete your existing signal data from within time cockpit using the "Delete Signals" wizard:</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/blob-sync-delete-signals.png" />
</p><p>Anyhow, to support this customer behavior, we improved the synchronization speed. While I hope that you do not have to initially synchronize time cockpit often, sometimes it is unavoidable, such as when setting up a new system.</p><h2>BLOB Synchronization</h2><p>To explain why the synchronization of those blobs took so long, consider that synchronization of those blobs used to happen sequentially:</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/blob-sync-sequential.PNG" />
</p><p>The time this process takes does not depend on the size of chunks, but mostly on latency: It takes some time for a HTTP request to reach windows azure blob storage and return the blob. Multiply that latency by the number of signal chunks (25000 after 3 ½ years of time cockpit for my user) and the reason for the slow sync process becomes clear.</p><p>Fortunately we fixed that situation: The obvious optimization is to do is to request multiple blobs concurrently and to therefore hide the latency associated with such a request. And that is exactly what the improved sync does now:</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/blob-sync-parallel.PNG" />
</p><p>The end result is that initial syncs taking multiple hours are now a thing of the past. The time cockpit environment I use for production now syncs in 11 minutes instead of 65 minutes.</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/blob-sync-improvement.png" />
</p><h2>Other Synchronization Improvements</h2><p>While working on the synchronization code we decided to include several other improvements besides the BLOB synchronization. Our main focus was on the initial synchronization for new users or devices. In this case we reduced the overall time required for synchronizing the data by not handling previously deleted elements and not looking at the local version information. These steps can be safely omitted for the initial creation of the local database.</p><p>An important fix which is not related to performance is the proper synchronization of a large number of changes containing cyclic relations. Some of our customers have configured entities containing relations on themselves. An example is the entity <strong>Task</strong> which could contain a relation <strong>Parent</strong> which is used to model hierarchies of issues or work items. To allow such a scenario the foreign key constraints of the cyclic relation had to be disabled or removed during the synchronization to avoid conflicts caused by data ordering issues. If a synchronization contained a large number of changes (larger than the synchronization batch size) this process sometimes failed when the constraint was reintroduced. This issue was resolved by not reintroducing the constraint until all batches for the entity have been processed.</p><h2>Example Results</h2><p>To get a better understanding of our synchronization performance and the speedup in version 1.14, we selected several customer accounts for benchmarking. The examples try to represent different scenarios where customers previously reported undesirable performance. All benchmarks were executed on a medium size Azure VM (2 cores, 3.5 GB memory) located in a different datacenter (to create a realistic latency between client and server database) using the local disk for storing the database and blobs. Each customer benchmark consists of an initial synchronization per time cockpit version to test.</p><h3>Customer A</h3><p>The first example database contains timesheets for <strong>5 people</strong> and <strong>4 years</strong> and the account used for synchronization contained <strong>signal data</strong> for <strong>3.5 years</strong> and a single device. This amounts to about <strong>22.000 timesheets</strong>, <strong>29.000 signal blobs</strong> and a total of <strong>57.000 operations</strong> for an initial synchronization.</p><p>The following chart shows the initial synchronization time in minutes for time cockpit versions <strong>June 2013 (1.13)</strong> and <strong>July 2013 (1.14)</strong>. The time is split into <strong>Data</strong> (Timesheets, Projects ...) and <strong>Signals</strong> (BLOBs).</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/SyncPerfCustomerA.png" title="Performance Customer A" />
</p><p>The <strong>total</strong> time improved from ~56 to ~17 minutes (<strong>-70%</strong>). Synchronization of <strong>general data</strong> changed from ~6 to ~4 minutes (<strong>-36%</strong>). The significant number of <strong>signal BLOBs</strong> showed the biggest improvement and changed from an unbearable ~50 to ~13 minutes (<strong>-74%</strong>).</p><h3>Customer B</h3><p>The next scenario shows an account <strong>without existing signal data</strong> but a significant number of <strong>timesheets (~42.000)</strong> and other updates which results in a total of <strong>~47.000 operations</strong>.</p><p>The initial synchronization time changed from ~7 to ~4 minutes (<strong>-41%</strong>).</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/SyncPerfCustomerB.png" title="Performance Customer B" />
</p><h3>Customer C</h3><p>The last example shows an account with a small to average number of <strong>signals (~5.000)</strong> and a very large number of <strong>timesheets (~81.000)</strong> and <strong>other data (~42.000)</strong> which results in a total of <strong>~128.000 operations</strong>.</p><p>The <strong>total</strong> time of the synchronization changed from ~24 to ~12 minutes (<strong>-50%</strong>). Of that time the <strong>general data</strong> phase improved from ~14 to ~9 minutes (<strong>-33%</strong>) while the <strong>signal</strong> synchronization changed from ~10 to ~3 minutes (<strong>-73%</strong>).</p><p>
  <img src="{{site.baseurl}}/content/images/blog/2013/06/SyncPerfCustomerC.png" title="Performance Customer C" />
</p><h2>Overall Result</h2><p>The overall/average improvement for all benchmarked data was a change of <strong>-62%</strong> in <strong>total</strong> synchronization time, <strong>-74%</strong> for <strong>signal</strong> synchronization time and <strong>-36%</strong> for <strong>general data</strong> synchronization.</p>